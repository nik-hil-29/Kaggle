{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7416971,"sourceType":"datasetVersion","datasetId":4314732},{"sourceId":7432876,"sourceType":"datasetVersion","datasetId":4325482},{"sourceId":7433018,"sourceType":"datasetVersion","datasetId":4325573}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:47:57.266371Z","iopub.execute_input":"2024-01-19T06:47:57.266796Z","iopub.status.idle":"2024-01-19T06:47:57.301245Z","shell.execute_reply.started":"2024-01-19T06:47:57.266755Z","shell.execute_reply":"2024-01-19T06:47:57.300082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport sys\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:47:57.303076Z","iopub.execute_input":"2024-01-19T06:47:57.303457Z","iopub.status.idle":"2024-01-19T06:48:04.546488Z","shell.execute_reply.started":"2024-01-19T06:47:57.303427Z","shell.execute_reply":"2024-01-19T06:48:04.545435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n# train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T07:17:22.893982Z","iopub.execute_input":"2024-01-19T07:17:22.894471Z","iopub.status.idle":"2024-01-19T07:17:22.907220Z","shell.execute_reply.started":"2024-01-19T07:17:22.894436Z","shell.execute_reply":"2024-01-19T07:17:22.906216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/new-valid-balanced-deep-fake/valid_strange.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T07:19:44.208323Z","iopub.execute_input":"2024-01-19T07:19:44.208832Z","iopub.status.idle":"2024-01-19T07:19:46.663071Z","shell.execute_reply.started":"2024-01-19T07:19:44.208796Z","shell.execute_reply":"2024-01-19T07:19:46.661742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T07:20:02.412493Z","iopub.execute_input":"2024-01-19T07:20:02.412952Z","iopub.status.idle":"2024-01-19T07:20:02.525389Z","shell.execute_reply.started":"2024-01-19T07:20:02.412920Z","shell.execute_reply":"2024-01-19T07:20:02.524293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"generated\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T07:20:56.248919Z","iopub.execute_input":"2024-01-19T07:20:56.249401Z","iopub.status.idle":"2024-01-19T07:20:56.261451Z","shell.execute_reply.started":"2024-01-19T07:20:56.249358Z","shell.execute_reply":"2024-01-19T07:20:56.260166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\ntrain = train[~(train['prompt_name'].isin(excluded_prompt_name_list))]\ntrain = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T07:21:10.156217Z","iopub.execute_input":"2024-01-19T07:21:10.156686Z","iopub.status.idle":"2024-01-19T07:21:10.189924Z","shell.execute_reply.started":"2024-01-19T07:21:10.156655Z","shell.execute_reply":"2024-01-19T07:21:10.189025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.text.values","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.858692Z","iopub.status.idle":"2024-01-19T06:48:05.859435Z","shell.execute_reply.started":"2024-01-19T06:48:05.859116Z","shell.execute_reply":"2024-01-19T06:48:05.859145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 14_000_000","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.860823Z","iopub.status.idle":"2024-01-19T06:48:05.861411Z","shell.execute_reply.started":"2024-01-19T06:48:05.861092Z","shell.execute_reply":"2024-01-19T06:48:05.861119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n\n\n# Adding normalization and pre_tokenizer\nraw_tokenizer.normalizer = normalizers.Sequence(\n    [normalizers.NFC()] + [normalizers.Lowercase()] \n    if LOWERCASE else []\n)\n\n\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# Adding special tokens and creating trainer instance\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(\n    vocab_size=VOCAB_SIZE, \n    special_tokens=special_tokens\n)\n\n\n# Creating huggingface dataset object\ndataset = Dataset.from_pandas(test[['text']])\n\ndef train_corp_iter():\n    \"\"\"\n    A generator function for iterating over a dataset in chunks.\n    \"\"\"    \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\n\n# Training from iterator REMEMBER it's training on test set...\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token  = \"[UNK]\",\n    pad_token  = \"[PAD]\",\n    cls_token  = \"[CLS]\",\n    sep_token  = \"[SEP]\",\n    mask_token = \"[MASK]\",\n)\n\n\n\n# Tokenize test set with new tokenizer\ntokenized_texts_test = []\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\n\n# Tokenize train set\ntokenized_texts_train = []\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.863073Z","iopub.status.idle":"2024-01-19T06:48:05.863653Z","shell.execute_reply.started":"2024-01-19T06:48:05.863360Z","shell.execute_reply":"2024-01-19T06:48:05.863386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_texts_test[1])\nprint()\nprint(tokenized_texts_test[2])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.865676Z","iopub.status.idle":"2024-01-19T06:48:05.866212Z","shell.execute_reply.started":"2024-01-19T06:48:05.865938Z","shell.execute_reply":"2024-01-19T06:48:05.865962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy(text):\n    \"\"\"\n    A dummy function to use as tokenizer for TfidfVectorizer. \n    It returns the text as it is since we already tokenized it.\n    \"\"\"\n    return text\n\n\n\n# Fitting TfidfVectoizer on test set\nvectorizer = TfidfVectorizer(\n    ngram_range   = (3, 5), \n    lowercase     = False, \n    sublinear_tf  = True, \n    analyzer      = 'word',\n    tokenizer     = dummy,\n    preprocessor  = dummy,\n    token_pattern = None, \n    strip_accents ='unicode')\n\n\nvectorizer.fit(tokenized_texts_test)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\nprint(vocab)\n\n\n# Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\nvectorizer = TfidfVectorizer(\n    ngram_range    = (3, 5), \n    lowercase      = False, \n    sublinear_tf   = True, \n    vocabulary     = vocab,\n    analyzer       = 'word',\n    tokenizer      = dummy,\n    preprocessor   = dummy,\n    token_pattern  = None, \n    strip_accents  ='unicode')\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\ndel vectorizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.867640Z","iopub.status.idle":"2024-01-19T06:48:05.868475Z","shell.execute_reply.started":"2024-01-19T06:48:05.868040Z","shell.execute_reply":"2024-01-19T06:48:05.868068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_label = train['generated'].values","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.871265Z","iopub.status.idle":"2024-01-19T06:48:05.871870Z","shell.execute_reply.started":"2024-01-19T06:48:05.871588Z","shell.execute_reply":"2024-01-19T06:48:05.871615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_train","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.873658Z","iopub.status.idle":"2024-01-19T06:48:05.874418Z","shell.execute_reply.started":"2024-01-19T06:48:05.873932Z","shell.execute_reply":"2024-01-19T06:48:05.873958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.876817Z","iopub.status.idle":"2024-01-19T06:48:05.877926Z","shell.execute_reply.started":"2024-01-19T06:48:05.877662Z","shell.execute_reply":"2024-01-19T06:48:05.877704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.879357Z","iopub.status.idle":"2024-01-19T06:48:05.880639Z","shell.execute_reply.started":"2024-01-19T06:48:05.880388Z","shell.execute_reply":"2024-01-19T06:48:05.880417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test.text.values) <= 5:\n    sub.to_csv('submission.csv', index=False)\nelse:\n    clf = MultinomialNB(alpha=0.0225)\n    \n    sgd_model = SGDClassifier(\n        max_iter     = 9000, \n        tol          = 1e-4, \n        random_state = 6743,\n        loss         = \"modified_huber\"\n    ) \n    \n    p={\n        'verbose'          : -1,\n        'n_iter'           : 3000,\n        'colsample_bytree' : 0.7800,\n        'colsample_bynode' : 0.8000, \n        'random_state'     : 6743,\n        'metric'           : 'auc',\n        'objective'        : 'cross_entropy',\n        'learning_rate'    : 0.00581909898961407, \n      }\n    lgb=LGBMClassifier(**p)\n    \n    \n    cat = CatBoostClassifier(\n        iterations        = 3000,\n        verbose           = 0,\n        subsample         = 0.35,\n        random_seed       = 6543,\n        allow_const_label = True,\n        loss_function     = 'CrossEntropy',\n        learning_rate     = 0.005599066836106983,\n    )\n    \n    \n    ensemble = VotingClassifier(\n        estimators = [('mnb', clf),\n                      ('sgd', sgd_model),\n                      ('lgb', lgb), \n                      ('cat', cat)],\n        weights    = [0.1, 0.31, 0.28, 0.67], \n        voting     = 'soft', \n        n_jobs     = -1\n    )\n    \n    ensemble.fit(tf_train, y_train_label)\n    gc.collect()\n    \n    final_preds = ensemble.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds\n    sub.to_csv('submission.csv', index=False)\n    sub.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T06:48:05.881890Z","iopub.status.idle":"2024-01-19T06:48:05.882861Z","shell.execute_reply.started":"2024-01-19T06:48:05.882640Z","shell.execute_reply":"2024-01-19T06:48:05.882662Z"},"trusted":true},"execution_count":null,"outputs":[]}]}