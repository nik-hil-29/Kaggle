{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381}],"dockerImageVersionId":30260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nikhilkushwaha2529/real-time-violence-detection-mobilenet-bi-lstm?scriptVersionId=178526855\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/nikhilkushwaha2529/real-time-violence-detection-mobilenet-bi-lstm?scriptVersionId=162488190\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"# **Importing the required libraries**","metadata":{"id":"ZsKYkocGm0La"}},{"cell_type":"code","source":"import os\nimport shutil\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow \nimport keras\nfrom collections import deque\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn\")\n\n%matplotlib inline\n \nfrom sklearn.model_selection import train_test_split\n \nfrom keras.layers import *\nfrom keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model","metadata":{"id":"KN35l9ZJ3aTh","execution":{"iopub.status.busy":"2024-02-11T10:12:20.078093Z","iopub.execute_input":"2024-02-11T10:12:20.078581Z","iopub.status.idle":"2024-02-11T10:12:28.08286Z","shell.execute_reply.started":"2024-02-11T10:12:20.078488Z","shell.execute_reply":"2024-02-11T10:12:28.081702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize the Data**","metadata":{"id":"2JxJyU0e3aTn"}},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\n\n# To Show a Video in Notebook\ndef Play_Video(filepath):\n    html = ''\n    video = open(filepath,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=640 muted controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n    return HTML(html)","metadata":{"id":"Sn7l5xJlmtrf","execution":{"iopub.status.busy":"2024-02-11T10:12:28.084635Z","iopub.execute_input":"2024-02-11T10:12:28.085274Z","iopub.status.idle":"2024-02-11T10:12:28.095418Z","shell.execute_reply.started":"2024-02-11T10:12:28.085238Z","shell.execute_reply":"2024-02-11T10:12:28.093271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classes Directories\nNonViolnceVideos_Dir = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/\"\nViolnceVideos_Dir = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/\"\n\n# Retrieve the list of all the video files present in the Class Directory.\nNonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)\nViolence_files_names_list = os.listdir(ViolnceVideos_Dir)\n\n# Randomly select a video file from the Classes Directory.\nRandom_NonViolence_Video = random.choice(NonViolence_files_names_list)\nRandom_Violence_Video = random.choice(Violence_files_names_list)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"WzJy3lsU3aTp","execution":{"iopub.status.busy":"2024-02-11T10:12:28.107907Z","iopub.execute_input":"2024-02-11T10:12:28.108705Z","iopub.status.idle":"2024-02-11T10:12:28.970866Z","shell.execute_reply.started":"2024-02-11T10:12:28.108655Z","shell.execute_reply":"2024-02-11T10:12:28.969869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Play Random Non Violence Video**","metadata":{"id":"pq5KrFGd-HJ6"}},{"cell_type":"code","source":"Play_Video(f\"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}\")","metadata":{"id":"P2wg6bCP9C3S","outputId":"cec080f1-ffc1-4343-8933-49b8ca755bbb","execution":{"iopub.status.busy":"2024-02-11T10:12:28.971972Z","iopub.execute_input":"2024-02-11T10:12:28.972659Z","iopub.status.idle":"2024-02-11T10:12:29.022078Z","shell.execute_reply.started":"2024-02-11T10:12:28.972617Z","shell.execute_reply":"2024-02-11T10:12:29.020653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Play Random Violence Video**","metadata":{"id":"eDllFj2y-LXj"}},{"cell_type":"code","source":"Play_Video(f\"{ViolnceVideos_Dir}/{Random_Violence_Video}\")","metadata":{"id":"AdbW5DIYnk2D","outputId":"8b2d7e38-ba3d-4e97-9458-89e16ff24c59","execution":{"iopub.status.busy":"2024-02-11T10:12:29.023533Z","iopub.execute_input":"2024-02-11T10:12:29.023928Z","iopub.status.idle":"2024-02-11T10:12:29.129355Z","shell.execute_reply.started":"2024-02-11T10:12:29.023895Z","shell.execute_reply":"2024-02-11T10:12:29.127819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Extracting Frames**","metadata":{"id":"ySOhHqy83aTq"}},{"cell_type":"code","source":"# Specify the height and width to which each video frame will be resized in our dataset.\nIMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n \n# Specify the number of frames of a video that will be fed to the model as one sequence.\nSEQUENCE_LENGTH = 16\n \n\nDATASET_DIR = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/\"\n \nCLASSES_LIST = [\"NonViolence\",\"Violence\"]","metadata":{"id":"1arHoOHI3aTr","execution":{"iopub.status.busy":"2024-02-11T10:12:29.130814Z","iopub.execute_input":"2024-02-11T10:12:29.131216Z","iopub.status.idle":"2024-02-11T10:12:29.13759Z","shell.execute_reply.started":"2024-02-11T10:12:29.131164Z","shell.execute_reply":"2024-02-11T10:12:29.136314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frames_extraction(video_path):\n \n    frames_list = []\n    \n    # Read the Video File\n    video_reader = cv2.VideoCapture(video_path)\n \n    # Get the total number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n \n    # Calculate the the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n \n    # Iterate through the Video Frames.\n    for frame_counter in range(SEQUENCE_LENGTH):\n \n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n \n        # Reading the frame from the video. \n        success, frame = video_reader.read() \n \n        if not success:\n            break\n \n        # Resize the Frame to fixed height and width.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame\n        normalized_frame = resized_frame / 255\n        \n        # Append the normalized frame into the frames list\n        frames_list.append(normalized_frame)\n    \n \n    video_reader.release()\n \n    return frames_list","metadata":{"id":"-yB8ePeC3aTs","execution":{"iopub.status.busy":"2024-02-11T10:12:29.139612Z","iopub.execute_input":"2024-02-11T10:12:29.140112Z","iopub.status.idle":"2024-02-11T10:12:29.156104Z","shell.execute_reply.started":"2024-02-11T10:12:29.140074Z","shell.execute_reply":"2024-02-11T10:12:29.154324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Creating the Data**","metadata":{"id":"uqAqiZxD3aTt"}},{"cell_type":"code","source":"def create_dataset():\n \n    features = []\n    labels = []\n    video_files_paths = []\n    \n    # Iterating through all the classes.\n    for class_index, class_name in enumerate(CLASSES_LIST):\n        \n        print(f'Extracting Data of Class: {class_name}')\n        \n        # Get the list of video files present in the specific class name directory.\n        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n        \n        # Iterate through all the files present in the files list.\n        for file_name in files_list:\n            \n            # Get the complete video path.\n            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n \n            # Extract the frames of the video file.\n            frames = frames_extraction(video_file_path)\n \n            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.\n            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n            if len(frames) == SEQUENCE_LENGTH:\n \n                # Append the data to their repective lists.\n                features.append(frames)\n                labels.append(class_index)\n                video_files_paths.append(video_file_path)\n \n    features = np.asarray(features)\n    labels = np.array(labels)  \n\n    return features, labels, video_files_paths","metadata":{"id":"vj_AQqju3aTu","execution":{"iopub.status.busy":"2024-02-11T10:12:29.160548Z","iopub.execute_input":"2024-02-11T10:12:29.161Z","iopub.status.idle":"2024-02-11T10:12:29.17199Z","shell.execute_reply.started":"2024-02-11T10:12:29.160959Z","shell.execute_reply":"2024-02-11T10:12:29.170547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset.\nfeatures, labels, video_files_paths = create_dataset()","metadata":{"id":"rgpokUY83aTv","execution":{"iopub.status.busy":"2024-02-11T10:12:29.173554Z","iopub.execute_input":"2024-02-11T10:12:29.173965Z","iopub.status.idle":"2024-02-11T10:36:00.737005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the extracted data\nnp.save(\"features.npy\",features)\nnp.save(\"labels.npy\",labels)\nnp.save(\"video_files_paths.npy\",video_files_paths)","metadata":{"id":"Hu8NKv4H3aTv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features, labels, video_files_paths = np.load(\"features.npy\") , np.load(\"labels.npy\") ,  np.load(\"video_files_paths.npy\")","metadata":{"id":"9KOPtXlH3aTw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Encoding and Splitting Training-Testing Sets**","metadata":{"id":"lkN8bgxH3aTw"}},{"cell_type":"code","source":"# convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"id":"SIN6V5GN3aTw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the Data into Train ( 90% ) and Test Set ( 10% ).\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1,\n                                                                            shuffle = True, random_state = 42)","metadata":{"id":"P0uFnKvq3aTx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features_train.shape,labels_train.shape )\nprint(features_test.shape, labels_test.shape)","metadata":{"id":"8X0LeeVW3aTx","outputId":"52c6e405-d4a1-4688-ef79-852689bd59f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing MobileNet and Fine-Tuning it.**","metadata":{"id":"bLGa3mfdtu_K"}},{"cell_type":"code","source":"from keras.applications.mobilenet_v2 import MobileNetV2\n\nmobilenet = MobileNetV2( include_top=False , weights=\"imagenet\")\n\n#Fine-Tuning to make the last 40 layer trainable\nmobilenet.trainable=True\n\nfor layer in mobilenet.layers[:-40]:\n  layer.trainable=False\n\n#mobilenet.summary()","metadata":{"id":"Tpo2Q-Uf3aT8","outputId":"fccd8ac7-0787-4b33-cf27-7a303ada1c0e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Building The Model**","metadata":{"id":"PFfDOTnZ3aTy"}},{"cell_type":"code","source":"def create_model():\n \n    model = Sequential()\n\n    ########################################################################################################################\n    \n    #Specifying Input to match features shape\n    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n    \n    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n    model.add(TimeDistributed(mobilenet))\n    \n    model.add(Dropout(0.25))\n                                    \n    model.add(TimeDistributed(Flatten()))\n\n    \n    lstm_fw = LSTM(units=32)\n    lstm_bw = LSTM(units=32, go_backwards = True)  \n\n    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n    \n    model.add(Dropout(0.25))\n\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(0.25))\n    \n    \n    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n \n    ########################################################################################################################\n \n    model.summary()\n    \n    return model","metadata":{"id":"CWtYR7bM3aT9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing the Model\nMoBiLSTM_model = create_model()\n\n# Plot the structure of the contructed LRCN model.\nplot_model(MoBiLSTM_model, to_file = 'MobBiLSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)","metadata":{"id":"auo9g9rpmBOS","outputId":"07c69b21-1e15-4481-cff0-fae9f9b1abef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Specifying Callbacks and Fitting**","metadata":{"id":"c_jvIgP9wEh6"}},{"cell_type":"code","source":"# Create Early Stopping Callback to monitor the accuracy\nearly_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)\n\n# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                  factor=0.6,\n                                                  patience=5,\n                                                  min_lr=0.00005,\n                                                  verbose=1)\n \n# Compiling the model \nMoBiLSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = [\"accuracy\"])\n \n# Fitting the model \nMobBiLSTM_model_history = MoBiLSTM_model.fit(x = features_train, y = labels_train, epochs = 30, batch_size = 8 ,\n                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback,reduce_lr])","metadata":{"id":"oA-QWlSV3aT_","outputId":"f1b4723e-1d04-4842-b5c6-29338569f694","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation_history = MoBiLSTM_model.evaluate(features_test, labels_test)","metadata":{"id":"6577H1to3aT_","outputId":"dd4650e7-7089-4931-d526-c98440b7210b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MoBiLSTM_model.save(\"model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Evaluation**","metadata":{"id":"lPGXI4C53aUA"}},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n    \n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n    \n    # Get the Epochs Count\n    epochs = range(len(metric_value_1))\n \n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'orange', label = metric_name_2)\n \n    plt.title(str(plot_name))\n \n    plt.legend()","metadata":{"id":"hyHkkNo93aUB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(MobBiLSTM_model_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"id":"-YIHQIr13aUB","outputId":"a038e7f7-56be-4c92-9ea3-0dd25de1a247","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(MobBiLSTM_model_history, 'accuracy', 'val_accuracy', 'Total Loss vs Total Validation Loss')","metadata":{"id":"JUeYKvq23aUB","outputId":"b19327aa-bc13-45ad-d7db-5fccedd0f2aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicting the Test Set**","metadata":{"id":"f8rpSPRGxzLM"}},{"cell_type":"code","source":"labels_predict = MoBiLSTM_model.predict(features_test)","metadata":{"id":"BCuJOPWn3aUC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoding the data to use in Metrics\nlabels_predict = np.argmax(labels_predict , axis=1)\nlabels_test_normal = np.argmax(labels_test , axis=1)","metadata":{"id":"6PtV_By23aUC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_test_normal.shape , labels_predict.shape","metadata":{"id":"HP-sdFAj3aUC","outputId":"96fbaf31-208f-4271-e83d-36b2b5cb07cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy Score","metadata":{"id":"JR45ydemygm6"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nAccScore = accuracy_score(labels_predict, labels_test_normal)\nprint('Accuracy Score is : ', AccScore)","metadata":{"id":"99mVAuNu3aUC","outputId":"fc39c654-e5bb-4977-c840-fa4695dde3c5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix","metadata":{"id":"FMtS_I_MyiY5"}},{"cell_type":"code","source":"import seaborn as sns \nfrom sklearn.metrics import confusion_matrix\n\nax= plt.subplot()\ncm=confusion_matrix(labels_test_normal, labels_predict)\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  \n\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['True', 'False']); ax.yaxis.set_ticklabels(['NonViolence', 'Violence']);","metadata":{"id":"xzg5AVTC3aUD","outputId":"d469d340-b976-4858-9740-b0ae9e3b9883","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classification Report","metadata":{"id":"zX8EOmSTypNq"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nClassificationReport = classification_report(labels_test_normal,labels_predict)\nprint('Classification Report is : \\n', ClassificationReport)","metadata":{"id":"pcWhopTm3aUD","outputId":"aef35268-bd38-4755-9096-9e5dcfb6244b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prediction Frame By Frame**","metadata":{"id":"p9kjBjoN3aUD"}},{"cell_type":"code","source":"def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):\n    \n    # Read from the video file.\n    video_reader = cv2.VideoCapture(video_file_path)\n \n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    # VideoWriter to store the output video in the disk.\n    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), \n                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n \n    # Declare a queue to store video frames.\n    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n \n    # Store the predicted class in the video.\n    predicted_class_name = ''\n \n    # Iterate until the video is accessed successfully.\n    while video_reader.isOpened():\n \n        ok, frame = video_reader.read() \n        \n        if not ok:\n            break\n \n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame \n        normalized_frame = resized_frame / 255\n \n        # Appending the pre-processed frame into the frames list.\n        frames_queue.append(normalized_frame)\n \n        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.\n        # Check if the number of frames in the queue are equal to the fixed sequence length.\n        if len(frames_queue) == SEQUENCE_LENGTH:                        \n \n            # Pass the normalized frames to the model and get the predicted probabilities.\n            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n \n            # Get the index of class with highest probability.\n            predicted_label = np.argmax(predicted_labels_probabilities)\n \n            # Get the class name using the retrieved index.\n            predicted_class_name = CLASSES_LIST[predicted_label]\n \n        # Write predicted class name on top of the frame.\n        if predicted_class_name == \"Violence\":\n            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n        else:\n            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)\n         \n        # Write The frame into the disk using the VideoWriter\n        video_writer.write(frame)                       \n        \n    video_reader.release()\n    video_writer.release()","metadata":{"id":"90aMcgLB3aUD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"default\")\n\n# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)\ndef show_pred_frames(pred_video_path): \n\n    plt.figure(figsize=(20,15))\n\n    video_reader = cv2.VideoCapture(pred_video_path)\n\n    # Get the number of frames in the video.\n    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Get Random Frames from the video then Sort it\n    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))\n        \n    for counter, random_index in enumerate(random_range, 1):\n        \n        plt.subplot(5, 4, counter)\n\n        # Set the current frame position of the video.  \n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)\n          \n        ok, frame = video_reader.read() \n\n        if not ok:\n          break \n\n        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n\n        plt.imshow(frame);ax.figure.set_size_inches(20,20);plt.tight_layout()\n                            \n    video_reader.release()","metadata":{"id":"RMzc9v5T3aUD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct the output video path.\ntest_videos_directory = 'test_videos'\nos.makedirs(test_videos_directory, exist_ok = True)\n \noutput_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'","metadata":{"id":"TLoHFIZG1nfx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_378.mp4\"\n\n# Perform Prediction on the Test Video.\npredict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n\n# Show random frames from the output video\nshow_pred_frames(output_video_file_path)","metadata":{"id":"zXBFNGU83aUE","outputId":"53ee1964-9144-4a75-994b-df86e3b71018","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"lYbKAUan3aUE","outputId":"6dd94528-4e86-4ec7-e5db-f8b27cde68ef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_1.mp4\"\n\n# Perform Prediction on the Test Video.\npredict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n\n# Show random frames from the output video\nshow_pred_frames(output_video_file_path)","metadata":{"id":"aNOREFYZevVa","outputId":"24e5d5d6-30db-44e8-f95d-4c00596fdf62","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"WqM0I5jueyKx","outputId":"f9bf5016-a3d9-4c49-baeb-c27033dd45df","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prediction For The Video**","metadata":{"id":"Q4-IYON63aUE"}},{"cell_type":"code","source":"def predict_video(video_file_path, SEQUENCE_LENGTH):\n \n    video_reader = cv2.VideoCapture(video_file_path)\n \n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    # Declare a list to store video frames we will extract.\n    frames_list = []\n    \n    # Store the predicted class in the video.\n    predicted_class_name = ''\n \n    # Get the number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n \n    # Calculate the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n \n    # Iterating the number of times equal to the fixed length of sequence.\n    for frame_counter in range(SEQUENCE_LENGTH):\n \n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n \n        success, frame = video_reader.read() \n \n        if not success:\n            break\n \n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame.\n        normalized_frame = resized_frame / 255\n        \n        # Appending the pre-processed frame into the frames list\n        frames_list.append(normalized_frame)\n \n    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n    predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n \n    # Get the index of class with highest probability.\n    predicted_label = np.argmax(predicted_labels_probabilities)\n \n    # Get the class name using the retrieved index.\n    predicted_class_name = CLASSES_LIST[predicted_label]\n    \n    # Display the predicted class along with the prediction confidence.\n    print(f'Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n        \n    video_reader.release()","metadata":{"id":"p4gjtz-73aUE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_276.mp4\"\n\n# Perform Single Prediction on the Test Video.\npredict_video(input_video_file_path, SEQUENCE_LENGTH)\n\n# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"O6DQ5V-b3aUF","outputId":"2509c4e5-de79-4431-d18f-7caf178b4d01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_23.mp4\"\n\n# Perform Single Prediction on the Test Video.\npredict_video(input_video_file_path, SEQUENCE_LENGTH)\n\n# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"nxwWGvUg3aUF","outputId":"576775a3-d10a-4782-85d7-d809d6a845f7","trusted":true},"execution_count":null,"outputs":[]}]}